name: Deploy to GCP Cloud Composer
on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-cov flake8 google-cloud-storage
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
    - name: Setup GCP Auth
      id: auth
      uses: google-github-actions/auth@v1
      with:
        credentials_json: ${{ secrets.GCP_SA_KEY }}
    - name: Create service account key file for direct tests
      run: |
        echo '${{ secrets.GCP_SA_KEY }}' > key.json
        echo "GCS_KEY_PATH=${{ github.workspace }}/key.json" >> $GITHUB_ENV
        echo "GCS_BUCKET_NAME=loblaw-bucket" >> $GITHUB_ENV
    - name: Lint with flake8
      run: |
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
    - name: Test with pytest
      run: |
        pytest --cov=./ --cov-report=xml
      env:
        GCS_KEY_PATH: ${{ github.workspace }}/key.json
        GCS_BUCKET_NAME: loblaw-bucket
    
  deploy:
    needs: test
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install google-cloud-storage
    - name: Setup GCP Auth
      id: auth
      uses: google-github-actions/auth@v1
      with:
        credentials_json: ${{ secrets.GCP_SA_KEY }}
    - name: Set up Cloud SDK
      uses: google-github-actions/setup-gcloud@v1
      with:
        project_id: loblaw-sales-project
    
    # Upload DAG file to Cloud Composer
    - name: Upload DAG to Cloud Storage
      run: |
        # Copy the DAG file from your custom directory
        gsutil -m cp ./airflow-docker/dags/Scripts/etl_training_dag.py gs://${{ secrets.COMPOSER_BUCKET }}/dags/
    
    # Upload Scripts directory with all necessary modules
    - name: Upload Python modules to Cloud Storage
      run: |
        # Upload all script files from your custom directory
        gsutil -m cp ./airflow-docker/dags/Scripts/etl.py gs://${{ secrets.COMPOSER_BUCKET }}/dags/Scripts/
        gsutil -m cp ./airflow-docker/dags/Scripts/Model_training.py gs://${{ secrets.COMPOSER_BUCKET }}/dags/Scripts/
        gsutil -m cp ./airflow-docker/dags/Scripts/evaluate.py gs://${{ secrets.COMPOSER_BUCKET }}/dags/Scripts/
        
        # Create and upload __init__.py file
        touch __init__.py
        gsutil -m cp ./__init__.py gs://${{ secrets.COMPOSER_BUCKET }}/dags/Scripts/
    
    # Upload data files to the raw directory in the loblaw-bucket
    - name: Upload raw data files to storage bucket
      run: |
        # Upload data files if they exist
        if [ -d "./airflow-docker/data" ]; then
          gsutil -m cp -r ./airflow-docker/data/* gs://loblaw-bucket/raw/
        elif [ -d "./data" ]; then
          gsutil -m cp -r ./data/* gs://loblaw-bucket/raw/
        fi
    
    # Trigger the DAG to run
    - name: Trigger DAG
      run: |
        gcloud composer environments run ${{ secrets.COMPOSER_ENV_NAME }} \
          --location ${{ secrets.COMPOSER_LOCATION }} \
          dags trigger -- parallel_etl_model_training